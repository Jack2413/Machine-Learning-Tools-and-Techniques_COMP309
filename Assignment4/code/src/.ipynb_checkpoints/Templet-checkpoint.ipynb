{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGD(0/49): loss=2836.3166979413654, w=0.6957215842170359, b=13.291070475617776\n",
      "BGD(1/49): loss=1816.0061925721427, w=1.2526080611836594, b=23.923926856111997\n",
      "BGD(2/49): loss=1163.0054833099957, w=1.6983647478578394, b=32.43021196050738\n",
      "BGD(3/49): loss=745.0837570411461, w=2.0551682112801557, b=39.23524004402368\n",
      "BGD(4/49): loss=477.6130370257672, w=2.34076956133509, b=44.67926251083672\n",
      "BGD(5/49): loss=306.4312539059417, w=2.569377575312399, b=49.03448048428716\n",
      "BGD(6/49): loss=196.8745780593404, w=2.7523655900560167, b=52.51865486304751\n",
      "BGD(7/49): loss=126.75809110353222, w=2.898837329857466, b=55.305994366055785\n",
      "BGD(8/49): loss=81.88340207435094, w=3.0160798202496504, b=57.53586596846241\n",
      "BGD(9/49): loss=53.163513076380234, w=3.109925920336907, b=59.31976325038771\n",
      "BGD(10/49): loss=34.782727722721496, w=3.18504450978453, b=60.746881075927945\n",
      "BGD(11/49): loss=23.018988963492884, w=3.245172767382384, b=61.888575336360134\n",
      "BGD(12/49): loss=15.490173006837233, w=3.2933020971307116, b=62.801930744705885\n",
      "BGD(13/49): loss=10.67171596163071, w=3.331826951742596, b=63.53261507138249\n",
      "BGD(14/49): loss=7.587893949061718, w=3.362663957589709, b=64.11716253272377\n",
      "BGD(15/49): loss=5.6142417719299775, w=3.3873472676033316, b=64.58480050179679\n",
      "BGD(16/49): loss=4.351100477218374, w=3.407104885974236, b=64.95891087705522\n",
      "BGD(17/49): loss=3.542687548965631, w=3.4229197618346805, b=65.25819917726196\n",
      "BGD(18/49): loss=3.025301673337975, w=3.435578691356752, b=65.49762981742735\n",
      "BGD(19/49): loss=2.694173686807719, w=3.445711461165308, b=65.68917432955966\n",
      "BGD(20/49): loss=2.482251117976182, w=3.4538221804654015, b=65.84240993926551\n",
      "BGD(21/49): loss=2.346620252686955, w=3.4603143606696096, b=65.96499842703018\n",
      "BGD(22/49): loss=2.259816229010502, w=3.4655109902464, b=66.06306921724193\n",
      "BGD(23/49): loss=2.204261480935146, w=3.469670603520978, b=66.14152584941132\n",
      "BGD(24/49): loss=2.1687063313735604, w=3.473000142857651, b=66.20429115514683\n",
      "BGD(25/49): loss=2.1459509646675894, w=3.4756652541222506, b=66.25450339973524\n",
      "BGD(26/49): loss=2.131387484493873, w=3.4777985276278254, b=66.29467319540598\n",
      "BGD(27/49): loss=2.122066828041937, w=3.4795060945538436, b=66.32680903194256\n",
      "BGD(28/49): loss=2.1161015892418784, w=3.4808729070132918, b=66.35251770117183\n",
      "BGD(29/49): loss=2.1122838244472413, w=3.4819669644530546, b=66.37308463655525\n",
      "BGD(30/49): loss=2.109840447314099, w=3.482842696652616, b=66.38953818486198\n",
      "BGD(31/49): loss=2.108276681038107, w=3.4835436716265757, b=66.40270102350736\n",
      "BGD(32/49): loss=2.1072758674750802, w=3.4841047631501767, b=66.41323129442367\n",
      "BGD(33/49): loss=2.106635344778814, w=3.484553885743068, b=66.42165551115671\n",
      "BGD(34/49): loss=2.1062254089615755, w=3.484913383427422, b=66.42839488454315\n",
      "BGD(35/49): loss=2.1059630492109824, w=3.485201141351654, b=66.4337863832523\n",
      "BGD(36/49): loss=2.1057951384403735, w=3.48543147558345, b=66.43809958221962\n",
      "BGD(37/49): loss=2.1056876752074625, w=3.485615845339657, b=66.44155014139348\n",
      "BGD(38/49): loss=2.1056188985207354, w=3.4857634230867363, b=66.44431058873256\n",
      "BGD(39/49): loss=2.105574881301768, w=3.4858815508745096, b=66.44651894660383\n",
      "BGD(40/49): loss=2.1055467101922773, w=3.485976105605967, b=66.44828563290085\n",
      "BGD(41/49): loss=2.1055286806249525, w=3.4860517914154583, b=66.44969898193845\n",
      "BGD(42/49): loss=2.1055171416651848, w=3.4861123737011885, b=66.45082966116854\n",
      "BGD(43/49): loss=2.1055097567074306, w=3.4861608664552333, b=66.45173420455261\n",
      "BGD(44/49): loss=2.105505030319411, w=3.4861996822108043, b=66.45245783925986\n",
      "BGD(45/49): loss=2.1055020054214295, w=3.486230752066708, b=66.45303674702568\n",
      "BGD(46/49): loss=2.105500069480542, w=3.4862556217602556, b=66.45349987323833\n",
      "BGD(47/49): loss=2.1054988304744127, w=3.486275528568291, b=66.45387037420845\n",
      "BGD(48/49): loss=2.105498037507953, w=3.4862914628621895, b=66.45416677498454\n",
      "BGD(49/49): loss=2.105497530007792, w=3.486304217379217, b=66.45440389560541\n",
      "Learn: execution time=0.012 seconds\n",
      "R2: 0.92\n",
      "MSE: 1.36\n",
      "RMSE: 1.17\n",
      "MAE: 0.94\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "This is an example to perform simple linear regression algorithm on the dataset (weight and height),\n",
    "where x = weight and y = height.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from utilities.losses import compute_loss\n",
    "from utilities.optimizers import gradient_descent, pso, mini_batch_gradient_descent\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# General settings\n",
    "from utilities.visualization import visualize_train, visualize_test\n",
    "\n",
    "seed = 0\n",
    "# Freeze the random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "train_test_split_test_size = 0.1\n",
    "\n",
    "# Training settings\n",
    "alpha = 0.1  # step size\n",
    "max_iters = 50  # max iterations\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Data from CSV\n",
    "    :return: df    a panda data frame\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"../data/Part 2 - Outliers/Part2.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def data_preprocess(data):\n",
    "    \"\"\"\n",
    "    Data preprocess:\n",
    "        1. Split the entire dataset into train and test\n",
    "        2. Split outputs and inputs\n",
    "        3. Standardize train and test\n",
    "        4. Add intercept dummy for computation convenience\n",
    "    :param data: the given dataset (format: panda DataFrame)\n",
    "    :return: train_data       train data contains only inputs\n",
    "             train_labels     train data contains only labels\n",
    "             test_data        test data contains only inputs\n",
    "             test_labels      test data contains only labels\n",
    "             train_data_full       train data (full) contains both inputs and labels\n",
    "             test_data_full       test data (full) contains both inputs and labels\n",
    "    \"\"\"\n",
    "    # Split the data into train and test\n",
    "    train_data, test_data = train_test_split(data, test_size = train_test_split_test_size)\n",
    "\n",
    "    # Pre-process data (both train and test)\n",
    "    train_data_full = train_data.copy()\n",
    "    train_data = train_data.drop([\"Height\"], axis = 1)\n",
    "    train_labels = train_data_full[\"Height\"]\n",
    "\n",
    "    test_data_full = test_data.copy()\n",
    "    test_data = test_data.drop([\"Height\"], axis = 1)\n",
    "    test_labels = test_data_full[\"Height\"]\n",
    "\n",
    "    # Standardize the inputs\n",
    "    train_mean = train_data.mean()\n",
    "    train_std = train_data.std()\n",
    "    #print(train_mean)\n",
    "    train_data = (train_data - train_mean) / train_std\n",
    "    test_data = (test_data - train_mean) / train_std\n",
    "\n",
    "    # Tricks: add dummy intercept to both train and test\n",
    "    train_data['intercept_dummy'] = pd.Series(1.0, index = train_data.index)\n",
    "    test_data['intercept_dummy'] = pd.Series(1.0, index = test_data.index)\n",
    "    return train_data, train_labels, test_data, test_labels, train_data_full, test_data_full\n",
    "\n",
    "\n",
    "def learn(y, x, theta, max_iters, alpha, optimizer_type = \"BGD\", metric_type = \"MSE\"):\n",
    "    \"\"\"\n",
    "    Learn to estimate the regression parameters (i.e., w and b)\n",
    "    :param y:                   train labels\n",
    "    :param x:                   train data\n",
    "    :param theta:               model parameter\n",
    "    :param max_iters:           max training iterations\n",
    "    :param alpha:               step size\n",
    "    :param optimizer_type:      optimizer type (default: Batch Gradient Descient): GD, SGD, MiniBGD or PSO\n",
    "    :param metric_type:         metric type (MSE, RMSE, R2, MAE). NOTE: MAE can't be optimized by GD methods.\n",
    "    :return: thetas              all updated model parameters tracked during the learning course\n",
    "             losses             all losses tracked during the learning course\n",
    "    \"\"\"\n",
    "    thetas = None\n",
    "    losses = None\n",
    "    if optimizer_type == \"BGD\":\n",
    "        thetas, losses = gradient_descent(y, x, theta, max_iters, alpha, metric_type)\n",
    "    elif optimizer_type == \"MiniBGD\":\n",
    "        thetas, losses = mini_batch_gradient_descent(y, x, theta, max_iters, alpha, metric_type, mini_batch_size = 10)\n",
    "    elif optimizer_type == \"PSO\":\n",
    "        thetas, losses = pso(y, x, theta, max_iters, 100, metric_type)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"[ERROR] The optimizer '{ot}' is not defined, please double check and re-run your program.\".format(\n",
    "                ot = optimizer_type))\n",
    "    return thetas, losses\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Settings\n",
    "    metric_type = \"MSE\"  # MSE, RMSE, MAE, R2\n",
    "    optimizer_type = \"BGD\"  # PSO, BGD\n",
    "\n",
    "    # Step 1: Load Data\n",
    "    data = load_data()\n",
    "\n",
    "    # Step 2: Preprocess the data\n",
    "    train_data, train_labels, test_data, test_labels, train_data_full, test_data_full = data_preprocess(data)\n",
    "    \n",
    "    # Step 3: Learning Start\n",
    "    theta = np.array([0.0, 0.0])  # Initialize model parameter\n",
    "\n",
    "    start_time = datetime.datetime.now()  # Track learning starting time\n",
    "    thetas, losses = learn(train_labels.values, train_data.values, theta, max_iters, alpha, optimizer_type, metric_type)\n",
    "    #print(train_data.values)\n",
    "    end_time = datetime.datetime.now()  # Track learning ending time\n",
    "    exection_time = (end_time - start_time).total_seconds()  # Track execution time\n",
    "\n",
    "    # Step 4: Results presentation\n",
    "    print(\"Learn: execution time={t:.3f} seconds\".format(t = exection_time))\n",
    "\n",
    "    # Build baseline model\n",
    "    print(\"R2: {:.2f}\".format(-compute_loss(test_labels.values, test_data.values, thetas[-1], \"R2\")))  # R2 should be maximize\n",
    "    print(\"MSE: {:.2f}\".format(compute_loss(test_labels.values, test_data.values, thetas[-1], \"MSE\")))\n",
    "    print(\"RMSE: {:.2f}\".format(compute_loss(test_labels.values, test_data.values, thetas[-1], \"RMSE\")))\n",
    "    print(\"MAE: {:.2f}\".format(compute_loss(test_labels.values, test_data.values, thetas[-1], \"MAE\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
