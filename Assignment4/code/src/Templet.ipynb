{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Weight\n",
      "363  0.003768\n",
      "170  0.012589\n",
      "343 -0.012347\n",
      "144 -0.189145\n",
      "132  0.102469\n",
      "12   0.347154\n",
      "327  0.207935\n",
      "173 -0.449929\n",
      "224  0.028814\n",
      "342  0.584673\n",
      "78  -0.302801\n",
      "276  1.163808\n",
      "387 -0.690117\n",
      "425  0.401918\n",
      "301  0.294028\n",
      "196 -1.173170\n",
      "10  -1.274103\n",
      "469  1.568309\n",
      "271  2.721050\n",
      "75   1.061448\n",
      "142 -1.225926\n",
      "65  -1.061197\n",
      "340 -1.036641\n",
      "484 -0.364475\n",
      "175  1.454409\n",
      "362 -0.377042\n",
      "264 -0.609621\n",
      "100 -0.835436\n",
      "491  0.875736\n",
      "295 -1.846477\n",
      "..        ...\n",
      "265  0.253572\n",
      "72  -1.247668\n",
      "333  1.101673\n",
      "25   1.543915\n",
      "165  0.869766\n",
      "337 -0.184267\n",
      "483 -0.381774\n",
      "174 -1.426161\n",
      "486 -0.621132\n",
      "39  -0.414343\n",
      "193 -0.558058\n",
      "314  0.278006\n",
      "396  0.709789\n",
      "88   0.950470\n",
      "472  0.526648\n",
      "70   1.410079\n",
      "87   0.693910\n",
      "292 -0.617225\n",
      "242 -0.126129\n",
      "277  0.456826\n",
      "211 -1.029849\n",
      "9    0.061343\n",
      "359 -2.378442\n",
      "195 -0.224789\n",
      "251  0.547196\n",
      "323 -1.357428\n",
      "192  1.491483\n",
      "117 -0.653297\n",
      "47  -0.173633\n",
      "172 -0.748870\n",
      "\n",
      "[450 rows x 1 columns]\n",
      "BGD(0/49): loss=129.91688658833777, w=5.794171897937854, b=32.479221647084444\n",
      "BGD(1/49): loss=103.93350927067019, w=10.432084603798312, b=58.462598964752004\n",
      "BGD(2/49): loss=83.14680741653613, w=14.144476063022598, b=79.24930081888604\n",
      "BGD(3/49): loss=66.51744593322888, w=17.116039182161668, b=95.87866230219328\n",
      "BGD(4/49): loss=53.213956746583094, w=19.49461037219253, b=109.18215148883905\n",
      "BGD(5/49): loss=42.57116539726647, w=21.398524466968333, b=119.82494283815568\n",
      "BGD(6/49): loss=34.06957024013345, w=22.922501926831092, b=128.33917591760897\n",
      "BGD(7/49): loss=27.319469783722848, w=24.142361218036786, b=135.1505623811716\n",
      "BGD(8/49): loss=22.062193702249917, w=25.118790810686317, b=140.5996715520217\n",
      "BGD(9/49): loss=18.24120457420902, w=25.90036845351378, b=144.9589588887018\n",
      "BGD(10/49): loss=15.50122323175853, w=26.525977935617007, b=148.44638875804586\n",
      "BGD(11/49): loss=13.632806546717168, w=27.0267435699583, b=151.23633265352112\n",
      "BGD(12/49): loss=12.366789621584353, w=27.427578639935483, b=153.4682877699013\n",
      "BGD(13/49): loss=11.525389099158929, w=27.748424844837217, b=155.25385186300548\n",
      "BGD(14/49): loss=10.941023274495157, w=28.005244407071892, b=156.68230313748882\n",
      "BGD(15/49): loss=10.544032637395585, w=28.21081419888729, b=157.82506415707547\n",
      "BGD(16/49): loss=10.284567709250481, w=28.375361396691527, b=158.7392729727448\n",
      "BGD(17/49): loss=10.114720293208524, w=28.50707228702283, b=159.47064002528026\n",
      "BGD(18/49): loss=9.995634620061974, w=28.612499537461353, b=160.05573366730863\n",
      "BGD(19/49): loss=9.916129822500027, w=28.696888194367922, b=160.52380858093133\n",
      "BGD(20/49): loss=9.858919466034925, w=28.76443662596291, b=160.89826851182949\n",
      "BGD(21/49): loss=9.818320616440031, w=28.818505392764056, b=161.197836456548\n",
      "BGD(22/49): loss=9.790407382325485, w=28.861784436767994, b=161.43749081232284\n",
      "BGD(23/49): loss=9.772202150445079, w=28.896426907101812, b=161.6292142969427\n",
      "BGD(24/49): loss=9.759811345202989, w=28.92415628002235, b=161.7825930846386\n",
      "BGD(25/49): loss=9.750712994087081, w=28.94635210252452, b=161.9052961147953\n",
      "BGD(26/49): loss=9.74378824328194, w=28.96411862533626, b=162.00345853892068\n",
      "BGD(27/49): loss=9.738696456170175, w=28.97833973981801, b=162.08198847822098\n",
      "BGD(28/49): loss=9.735017182676849, w=28.989722951898738, b=162.14481242966121\n",
      "BGD(29/49): loss=9.732544310288935, w=28.998834580768687, b=162.1950715908134\n",
      "BGD(30/49): loss=9.730670217350971, w=29.00612793347748, b=162.23527891973515\n",
      "BGD(31/49): loss=9.729170824867364, w=29.011965857134605, b=162.26744478287253\n",
      "BGD(32/49): loss=9.727971216321388, w=29.01663879069304, b=162.29317747338246\n",
      "BGD(33/49): loss=9.727011453795306, w=29.02037921439915, b=162.3137636257904\n",
      "BGD(34/49): loss=9.72624358318936, w=29.023373215774573, b=162.33023254771675\n",
      "BGD(35/49): loss=9.72567295355823, w=29.02576974754219, b=162.34340768525783\n",
      "BGD(36/49): loss=9.725226803341954, w=29.02768803808151, b=162.35394779529068\n",
      "BGD(37/49): loss=9.724869851439712, w=29.029223523086543, b=162.36237988331698\n",
      "BGD(38/49): loss=9.724584264520438, w=29.03045259352835, b=162.369125553738\n",
      "BGD(39/49): loss=9.724355774655749, w=29.031436396135323, b=162.37452209007483\n",
      "BGD(40/49): loss=9.724172966491546, w=29.032223875466507, b=162.37883931914428\n",
      "BGD(41/49): loss=9.724026706934989, w=29.032854208922267, b=162.38229310239984\n",
      "BGD(42/49): loss=9.723909688863799, w=29.033358755835078, b=162.38505612900428\n",
      "BGD(43/49): loss=9.723816066061461, w=29.0337626176084, b=162.38726655028785\n",
      "BGD(44/49): loss=9.723741161139568, w=29.034085886521176, b=162.3890348873147\n",
      "BGD(45/49): loss=9.723681231855068, w=29.03434464532647, b=162.39044955693618\n",
      "BGD(46/49): loss=9.723633284147505, w=29.03455176737462, b=162.39158129263336\n",
      "BGD(47/49): loss=9.723594922555577, w=29.03471755706738, b=162.39248668119112\n",
      "BGD(48/49): loss=9.723564230539816, w=29.0348502625059, b=162.39321099203733\n",
      "BGD(49/49): loss=9.723539674732212, w=29.034956485836908, b=162.39379044071427\n",
      "Learn: execution time=0.082 seconds\n",
      "R2: 0.92\n",
      "MSE: 99.15\n",
      "RMSE: 9.96\n",
      "MAE: 8.44\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "This is an example to perform simple linear regression algorithm on the dataset (weight and height),\n",
    "where x = weight and y = height.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from utilities.losses import compute_loss\n",
    "from utilities.optimizers import gradient_descent, pso, mini_batch_gradient_descent\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# General settings\n",
    "from utilities.visualization import visualize_train, visualize_test\n",
    "\n",
    "seed = 0\n",
    "# Freeze the random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "train_test_split_test_size = 0.1\n",
    "\n",
    "# Training settings\n",
    "alpha = 0.1  # step size\n",
    "max_iters = 50  # max iterations\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Data from CSV\n",
    "    :return: df    a panda data frame\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"../data/Part 3 - Outliers/Part2.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def data_preprocess(data):\n",
    "    \"\"\"\n",
    "    Data preprocess:\n",
    "        1. Split the entire dataset into train and test\n",
    "        2. Split outputs and inputs\n",
    "        3. Standardize train and test\n",
    "        4. Add intercept dummy for computation convenience\n",
    "    :param data: the given dataset (format: panda DataFrame)\n",
    "    :return: train_data       train data contains only inputs\n",
    "             train_labels     train data contains only labels\n",
    "             test_data        test data contains only inputs\n",
    "             test_labels      test data contains only labels\n",
    "             train_data_full       train data (full) contains both inputs and labels\n",
    "             test_data_full       test data (full) contains both inputs and labels\n",
    "    \"\"\"\n",
    "    # Split the data into train and test\n",
    "    train_data, test_data = train_test_split(data, test_size = train_test_split_test_size)\n",
    "\n",
    "    # Pre-process data (both train and test)\n",
    "    train_data_full = train_data.copy()\n",
    "    train_data = train_data.drop([\"Height\"], axis = 1)\n",
    "    train_labels = train_data_full[\"Height\"]\n",
    "\n",
    "    test_data_full = test_data.copy()\n",
    "    test_data = test_data.drop([\"Height\"], axis = 1)\n",
    "    test_labels = test_data_full[\"Height\"]\n",
    "    \n",
    "\n",
    "    # Standardize the inputs\n",
    "    train_mean = train_data.mean()\n",
    "    train_std = train_data.std()\n",
    "    train_data = (train_data - train_mean) / train_std\n",
    "    test_data = (test_data - train_mean) / train_std\n",
    "    \n",
    "    print(train_data)\n",
    "\n",
    "    # Tricks: add dummy intercept to both train and test\n",
    "    train_data['intercept_dummy'] = pd.Series(1.0, index = train_data.index)\n",
    "    test_data['intercept_dummy'] = pd.Series(1.0, index = test_data.index)\n",
    "    return train_data, train_labels, test_data, test_labels, train_data_full, test_data_full\n",
    "\n",
    "\n",
    "def learn(y, x, theta, max_iters, alpha, optimizer_type = \"BGD\", metric_type = \"MSE\"):\n",
    "    \"\"\"\n",
    "    Learn to estimate the regression parameters (i.e., w and b)\n",
    "    :param y:                   train labels\n",
    "    :param x:                   train data\n",
    "    :param theta:               model parameter\n",
    "    :param max_iters:           max training iterations\n",
    "    :param alpha:               step size\n",
    "    :param optimizer_type:      optimizer type (default: Batch Gradient Descient): GD, SGD, MiniBGD or PSO\n",
    "    :param metric_type:         metric type (MSE, RMSE, R2, MAE). NOTE: MAE can't be optimized by GD methods.\n",
    "    :return: thetas              all updated model parameters tracked during the learning course\n",
    "             losses             all losses tracked during the learning course\n",
    "    \"\"\"\n",
    "    thetas = None\n",
    "    losses = None\n",
    "    if optimizer_type == \"BGD\":\n",
    "        thetas, losses = gradient_descent(y, x, theta, max_iters, alpha, metric_type)\n",
    "    elif optimizer_type == \"MiniBGD\":\n",
    "        thetas, losses = mini_batch_gradient_descent(y, x, theta, max_iters, alpha, metric_type, mini_batch_size = 10)\n",
    "    elif optimizer_type == \"PSO\":\n",
    "        thetas, losses = pso(y, x, theta, max_iters, 100, metric_type)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"[ERROR] The optimizer '{ot}' is not defined, please double check and re-run your program.\".format(\n",
    "                ot = optimizer_type))\n",
    "    return thetas, losses\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Settings\n",
    "    metric_type = \"MAE\"  # MSE, RMSE, MAE, R2\n",
    "    optimizer_type = \"BGD\"  # PSO, BGD\n",
    "\n",
    "    # Step 1: Load Data\n",
    "    data = load_data()\n",
    "\n",
    "    # Step 2: Preprocess the data\n",
    "    train_data, train_labels, test_data, test_labels, train_data_full, test_data_full = data_preprocess(data)\n",
    "    \n",
    "    # Step 3: Learning Start\n",
    "    theta = np.array([0.0, 0.0])  # Initialize model parameter\n",
    "\n",
    "    start_time = datetime.datetime.now()  # Track learning starting time\n",
    "    thetas, losses = learn(train_labels.values, train_data.values, theta, max_iters, alpha, optimizer_type, metric_type)\n",
    "    #print(train_data.values)\n",
    "    end_time = datetime.datetime.now()  # Track learning ending time\n",
    "    exection_time = (end_time - start_time).total_seconds()  # Track execution time\n",
    "\n",
    "    # Step 4: Results presentation\n",
    "    print(\"Learn: execution time={t:.3f} seconds\".format(t = exection_time))\n",
    "\n",
    "    # Build baseline model\n",
    "    print(\"R2: {:.2f}\".format(-compute_loss(test_labels.values, test_data.values, thetas[-1], \"R2\")))  # R2 should be maximize\n",
    "    print(\"MSE: {:.2f}\".format(compute_loss(test_labels.values, test_data.values, thetas[-1], \"MSE\")))\n",
    "    print(\"RMSE: {:.2f}\".format(compute_loss(test_labels.values, test_data.values, thetas[-1], \"RMSE\")))\n",
    "    print(\"MAE: {:.2f}\".format(compute_loss(test_labels.values, test_data.values, thetas[-1], \"MAE\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
